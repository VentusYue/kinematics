# CCAåˆ†æç¨‹åºå¯¹æ¯”ï¼šLegacy vs Current

**ä½œè€…**: Analysis  
**æ—¥æœŸ**: 2025-12-21  
**ç›®çš„**: å¯¹æ¯”åˆ†ælegacyå®éªŒç¨‹åº(e_r67_cca_compare.py)ä¸å½“å‰CCAåˆ†æç¨‹åº(analysis/cca_alignment.py)çš„å·®å¼‚

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

Legacyç¨‹åºåœ¨ç›¸å…³æ•°æ®ä¸Šè¡¨ç°ä¼˜å¼‚ï¼ŒCCAç›¸å…³æ€§ç»“æœå‡†ç¡®å¯é ã€‚é€šè¿‡å¯¹æ¯”åˆ†æå‘ç°**ä¸‰ä¸ªå…³é”®å·®å¼‚**ï¼š

1. **PCAé¢„å¤„ç†**ï¼ˆLegacyæœ‰ï¼ŒCurrentæ— ï¼‰ - **æœ€å…³é”®**
2. **å™ªå£°æ³¨å…¥**ï¼ˆLegacyæœ‰ï¼ŒCurrentæ— ï¼‰ - **é‡è¦**
3. **Ridgeå¯¹é½æ–¹å¼**ï¼ˆLegacyå¯¹é½åˆ°gridä¸­å¿ƒï¼ŒCurrentå¯¹é½åˆ°åŸç‚¹ï¼‰

è¿™äº›å·®å¼‚å¯¼è‡´Currentç¨‹åºå‡ºç°ridge embeddingé«˜åº¦ç›¸ä¼¼(cosine sim=0.9989)çš„é—®é¢˜ã€‚

---

## ğŸ”¬ è¯¦ç»†å·®å¼‚å¯¹æ¯”

### 1. æ•°æ®é¢„å¤„ç†æµç¨‹

#### Legacyç¨‹åºï¼ˆe_r67_cca_compare.pyï¼‰âœ…

```python
# æ­¥éª¤1: æ·»åŠ å°å™ªå£°ï¼ˆregularizationï¼‰
noise = np.random.uniform(-0.001, 0.001, size=ridge_images.shape)
ridge_images = ridge_images + noise

noise = np.random.uniform(-0.001, 0.001, size=rnn_center_mat.shape)
rnn_center_mat = rnn_center_mat + noise

# æ­¥éª¤2: PCAé™ç»´
pca = PCA()
pca.fit(train_view_1)
train_view_1 = pca.transform(train_view_1)  # Hidden states

pca.fit(train_view_2)
train_view_2 = pca.transform(train_view_2)  # Ridge embeddings

# æ­¥éª¤3: CCA
A, B, r, U, V = canoncorr(train_view_1, train_view_2, fullReturn=True)
```

**ä¼˜ç‚¹**ï¼š
- âœ… PCAæå–ä¸»æˆåˆ†ï¼Œå»é™¤å†—ä½™ç»´åº¦
- âœ… å°å™ªå£°æ‰“ç ´æ•°å€¼å…±çº¿æ€§
- âœ… æé«˜CCAçš„æ•°å€¼ç¨³å®šæ€§

#### Currentç¨‹åºï¼ˆanalysis/cca_alignment.pyï¼‰

```python
# ç›´æ¥è¿è¡ŒCCAï¼Œæ— é¢„å¤„ç†
X = np.concatenate(X_samples, axis=0).astype(np.float32)
Y = np.concatenate(Y_samples, axis=0).astype(np.float32)

A, B, r, U, V = canoncorr(X, Y, fullReturn=True)
```

**é—®é¢˜**ï¼š
- âŒ æ— PCAé™ç»´ â†’ ridgeçš„441ç»´ä¸­å¯èƒ½æœ‰å¤§é‡å†—ä½™
- âŒ æ— å™ªå£°regularization â†’ å®¹æ˜“äº§ç”Ÿæ•°å€¼artifacts
- âŒ ç›´æ¥åœ¨é«˜ç»´ç©ºé—´CCA â†’ å¯èƒ½overfitting

---

### 2. Ridge Embeddingå®ç°å·®å¼‚

#### Legacy: ä½¿ç”¨JAX + å¯¹é½åˆ°Gridä¸­å¿ƒ

```python
@jax.jit
def build_ridge(A):
    # å…³é”®ï¼šå¯¹é½åˆ°21Ã—21 gridçš„ä¸­å¿ƒ (10, 10)
    A = A - A[0] + jnp.array([10, 10])
    
    # JAXåŠ é€Ÿçš„è¾å°„åœºè®¡ç®—
    imgs = jax.vmap(build_radiance_field)(A)
    img = get_max_radiance_field(imgs)
    return img
```

**å‚æ•°**ï¼š
- Grid size: 21Ã—21
- Center: (10, 10)
- è¾å°„åŠå¾„: 21 Ã— 1.414 â‰ˆ 29.7
- Value range: [0, 10]

#### Current: NumPy + å¯¹é½åˆ°åŸç‚¹

```python
# åœ¨cca_alignment.pyä¸­
path_tile = path_tile - path_tile[0]  # å¯¹é½åˆ°(0, 0)

# åœ¨ridge_embedding.pyä¸­
def build_ridge(path, grid_size=21):
    A = path.copy()
    center = grid_size // 2  # = 10
    offset = np.array([center, center]) - A[0]
    A = A + offset  # å®é™…ä¸Šä¹Ÿå¯¹é½åˆ°(10, 10)
```

**å‘ç°**ï¼š
- `ridge_embedding.py`å†…éƒ¨**å·²ç»**å¯¹é½åˆ°(10, 10)ï¼
- ä½†`cca_alignment.py`å…ˆå¯¹é½åˆ°(0, 0)
- è¿™å¯¼è‡´**åŒé‡å¯¹é½**ï¼šå…ˆ(0,0)å†(10,10)
- æœ€ç»ˆæ•ˆæœï¼šæ‰€æœ‰Ridgeå›¾åƒéƒ½éå¸¸ç›¸ä¼¼ï¼ˆéƒ½æ˜¯ä»(10,10)å¼€å§‹çš„çŸ­è·¯å¾„ï¼‰

---

### 3. æ ‡å‡†åŒ–å¤„ç†å·®å¼‚

#### Legacy: ç®€å•æ ‡å‡†åŒ–

```python
X = (X0 - np.mean(X0, 0)) / np.std(X0, 0)
Y = (Y0 - np.mean(Y0, 0)) / np.std(Y0, 0)
# ä¸å¤„ç†std=0çš„æƒ…å†µï¼ˆå‡è®¾æœ‰å™ªå£°ä¸ä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ï¼‰
```

#### Current: å®‰å…¨æ ‡å‡†åŒ–

```python
X_std = np.std(X0, 0)
Y_std = np.std(Y0, 0)

# å¤„ç†å¸¸é‡åˆ—
X_std[X_std == 0] = 1.0
Y_std[Y_std == 0] = 1.0

X = (X0 - np.mean(X0, 0)) / X_std
Y = (Y0 - np.mean(Y0, 0)) / Y_std
```

**è¯„ä»·**ï¼šCurrentåœ¨è¿™æ–¹é¢æ›´robustï¼Œè¿™ä¸æ˜¯é—®é¢˜æ‰€åœ¨

---

### 4. æ•°æ®ç»„ç»‡æ–¹å¼

#### Legacy: Cycle-level aggregation

```python
# æ¯ä¸ªæ ·æœ¬ = ä¸€ä¸ªå®Œæ•´çš„è·¯å¾„/behavior
# ä½¿ç”¨å¡«å……(padding)ç»Ÿä¸€é•¿åº¦
def preprocess(trjs, max_length):
    if trj.shape[0] < max_length:
        # ç”¨æœ€åä¸€ä¸ªå…ƒç´ å¡«å……
        processed_trj = np.concatenate([trj, np.repeat(last_element, ...)])

# ç»“æœï¼š(N_cycles, unified_length, feature_dim)
```

#### Current: Timestep-level concatenation

```python
# æ¯ä¸ªæ ·æœ¬ = ä¸€ä¸ªtimestep
X_samples.append(h_cycle)  # (L, H)
Y_samples.append(np.tile(ridge_vec, (L, 1)))  # (L, 441)

X = np.concatenate(X_samples, axis=0)  # (Î£L, H)
```

**é—®é¢˜**ï¼š
- Legacy: æ ·æœ¬é—´ç‹¬ç«‹ï¼ˆä¸åŒbehaviorsï¼‰
- Current: åŒä¸€cycleçš„timestepséƒ½ç›¸ä¼¼ â†’ äº§ç”Ÿå¤§é‡é‡å¤/ç›¸ä¼¼æ ·æœ¬
- **è¿™è¿›ä¸€æ­¥åŠ å‰§ridgeç›¸ä¼¼åº¦é—®é¢˜**

---

## ğŸ¯ æ”¹è¿›å»ºè®®

### â­â­â­ å…³é”®æ”¹è¿›ï¼ˆå¿…é¡»å®æ–½ï¼‰

#### æ”¹è¿›1: æ·»åŠ PCAé¢„å¤„ç†

```python
from sklearn.decomposition import PCA

# åœ¨è¿è¡ŒCCAä¹‹å‰
print("\n" + "-"*40)
print("PCA PREPROCESSING")
print("-"*40)

# PCA for X (Neural states)
pca_x = PCA()
pca_x.fit(X)
X_pca = pca_x.transform(X)
print(f"  X: {X.shape} â†’ {X_pca.shape}")
print(f"  X explained variance ratio (top 10): {pca_x.explained_variance_ratio_[:10]}")

# PCA for Y (Ridge embeddings)
pca_y = PCA()
pca_y.fit(Y)
Y_pca = pca_y.transform(Y)
print(f"  Y: {Y.shape} â†’ {Y_pca.shape}")
print(f"  Y explained variance ratio (top 10): {pca_y.explained_variance_ratio_[:10]}")

# ä½¿ç”¨PCA transformedæ•°æ®è¿›è¡ŒCCA
A, B, r, U, V = canoncorr(X_pca, Y_pca, fullReturn=True)
```

#### æ”¹è¿›2: æ·»åŠ å™ªå£°æ³¨å…¥

```python
# åœ¨concatenateä¹‹åï¼ŒCCAä¹‹å‰
print("  Adding regularization noise...")

noise_x = np.random.uniform(-0.001, 0.001, X.shape)
noise_y = np.random.uniform(-0.001, 0.001, Y.shape)

X = X + noise_x
Y = Y + noise_y
```

### â­â­ é‡è¦æ”¹è¿›

#### æ”¹è¿›3: ä¿®æ­£Ridgeå¯¹é½é€»è¾‘

**é—®é¢˜**ï¼šåŒé‡å¯¹é½å¯¼è‡´æ‰€æœ‰è·¯å¾„è¿‡äºç›¸ä¼¼

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# åœ¨cca_alignment.pyä¸­
# é€‰é¡¹A: ä¸åšé¢„å¯¹é½ï¼Œè®©ridge_embedding.pyå¤„ç†
# path_tile = path_xy / est_grid_step
# # æ³¨é‡Šæ‰ï¼špath_tile = path_tile - path_tile[0]

# é€‰é¡¹B: ç¡®ä¿åªå¯¹é½ä¸€æ¬¡
# æ£€æŸ¥ridge_embedding.pyçš„å®ç°ï¼Œå¦‚æœå®ƒå·²ç»å¯¹é½ï¼Œå°±ä¸è¦é¢„å¯¹é½
```

### â­ å¯é€‰æ”¹è¿›

#### æ”¹è¿›4: Cycle-level aggregation

è€ƒè™‘å…ˆå¯¹æ¯ä¸ªcycleå–å¹³å‡hidden stateï¼Œå†è¿›è¡ŒCCAï¼š

```python
# ä¿®æ”¹æ•°æ®æ„å»ºæ–¹å¼
cycle_hidden_means = []
cycle_ridge_vecs = []

for i in range(num_cycles):
    h_cycle = cycles_hidden[i]  # (L, H)
    h_mean = np.mean(h_cycle, axis=0)  # (H,)
    cycle_hidden_means.append(h_mean)
    
    ridge_vec = build_ridge_vector(path_tile)
    cycle_ridge_vecs.append(ridge_vec)

X = np.array(cycle_hidden_means)  # (N_cycles, H)
Y = np.array(cycle_ridge_vecs)     # (N_cycles, 441)
```

---

## ğŸ“Š å®æ–½è®¡åˆ’ç¤ºä¾‹

### å¿«é€Ÿæµ‹è¯•æ–¹æ¡ˆ

```python
# analysis/cca_alignment.py ä¿®æ”¹ä½ç½®
# åœ¨ "RUN CCA" sectionä¹‹å‰æ·»åŠ ï¼š

# =========================================================================
# PCA PREPROCESSING (Legacy-inspired improvement)
# =========================================================================
print("\n" + "-"*40)
print("PCA PREPROCESSING")
print("-"*40)

# Add noise for regularization
print("  Adding noise regularization...")
noise_x = np.random.uniform(-0.001, 0.001, X.shape)
noise_y = np.random.uniform(-0.001, 0.001, Y.shape)
X = X.astype(np.float64) + noise_x
Y = Y.astype(np.float64) + noise_y

# PCA transformation
from sklearn.decomposition import PCA

pca_x = PCA()
X_pca = pca_x.fit_transform(X)
print(f"  X PCA: {X.shape} â†’ {X_pca.shape}")
print(f"  X variance explained (cumsum): {pca_x.explained_variance_ratio_.cumsum()[:20]}")

pca_y = PCA()
Y_pca = pca_y.fit_transform(Y)
print(f"  Y PCA: {Y.shape} â†’ {Y_pca.shape}")
print(f"  Y variance explained (cumsum): {pca_y.explained_variance_ratio_.cumsum()[:20]}")

# Use PCA-transformed data for CCA
X = X_pca
Y = Y_pca

# Continue with CCA...
```

---

## ğŸ§ª é¢„æœŸç»“æœå¯¹æ¯”

### Before (CurrentçŠ¶æ€)

```
Ridge Embedding Diversity:
  Pairwise cosine similarity: mean=0.9989  âš ï¸ è¿‡é«˜ï¼
  
CCA Results:
  Top 10 correlations: [0.995, 0.971, 0.962, ...]
  High (>0.9): 5
  [WARN] Ridge embeddings are very similar!
```

### After (æ·»åŠ PCA+å™ªå£°å)

```
Ridge Embedding Diversity:
  Pairwise cosine similarity: mean=0.75-0.90  âœ… åˆç†èŒƒå›´
  
CCA Results:
  Top 10 correlations: [0.92, 0.85, 0.78, ...]
  High (>0.9): 2-3
  Medium (0.5-0.9): 6-8
  Distributionæ›´åŠ spread out - è¯´æ˜æ‰¾åˆ°äº†meaningful modes
```

---

## ğŸ” æŠ€æœ¯æ·±å…¥ï¼šä¸ºä»€ä¹ˆPCAæœ‰æ•ˆï¼Ÿ

### Ridge Embeddingçš„æœ¬è´¨ç‰¹æ€§

Ridge embeddings (441ç»´) å®é™…ä¸Šæ˜¯**ä½ç§©æµå½¢**ï¼š

1. **è·¯å¾„æ‹“æ‰‘çº¦æŸ**ï¼š
   - åœ¨21Ã—21 gridä¸­ï¼Œæœ‰æ„ä¹‰çš„è·¯å¾„æ¨¡å¼æ•°é‡ << 441
   - å¤§éƒ¨åˆ†varianceé›†ä¸­åœ¨å‰k<<441ä¸ªä¸»æˆåˆ†

2. **è¾å°„åœºé‡å **ï¼š
   - ç›¸é‚»ç‚¹çš„è¾å°„åœºé«˜åº¦é‡å 
   - åˆ›å»ºç»´åº¦é—´çš„å¼ºç›¸å…³æ€§

3. **å¯¹é½æ•ˆåº”**ï¼š
   - æ‰€æœ‰è·¯å¾„å¯¹é½åˆ°åŒä¸€èµ·ç‚¹
   - è¿›ä¸€æ­¥å‡å°‘variationç©ºé—´

### PCAçš„ä½œç”¨

```
åŸå§‹Ridgeç©ºé—´(441ç»´):
  ç»´åº¦1-50:  çœŸæ­£çš„è·¯å¾„å½¢çŠ¶ä¿¡æ¯ (90%+ variance)
  ç»´åº¦51-441: å™ªå£°ã€å†—ä½™ã€æ•°å€¼è¯¯å·® (~0% variance)

PCA after:
  ä¸»æˆåˆ†1-50: æ•è·çœŸå®å·®å¼‚
  å…¶ä½™æˆåˆ†:   è¢«è¿‡æ»¤
  
â†’ CCAåœ¨clean feature spaceä¸Šå·¥ä½œï¼Œç»“æœæ›´meaningful
```

---

## ğŸ“ˆ å®é™…æ¡ˆä¾‹åˆ†æ

### Legacyç¨‹åºçš„æ•°æ®ç‰¹å¾

```python
# Legacyå¤„ç†çš„æ•°æ®
N_cycles = ~4000
Sequence lengths: 5-14 (å¡«å……åˆ°ç»Ÿä¸€é•¿åº¦)
Hidden dim: 128
Ridge dim: 441

ç»è¿‡PCAå:
X_pca: (4000, 128) - ä¿ç•™æ‰€æœ‰128ç»´ï¼ˆå·²ç»è¾ƒä½ï¼‰
Y_pca: (4000, ~50-100) - æ˜¾è‘—é™ç»´ï¼ˆä»441â†’ä¸»è¦variance componentsï¼‰

CCAç»“æœ: æ¸…æ™°çš„mode separation
```

### Currentç¨‹åºçš„æ•°æ®ç‰¹å¾

```python
# Currentå¤„ç†çš„æ•°æ®  
N_samples =æ³¨å†Œ 112,272 timesteps (from 16,284 cycles)
Hidden dim: 256
Ridge dim: 441

é«˜åº¦é‡å¤ï¼š
- æ¯ä¸ªcycleè´¡çŒ®å¹³å‡6.9ä¸ªtimesteps
- è¿™6.9ä¸ªtimestepsçš„ridge embeddingå®Œå…¨ç›¸åŒï¼ˆåŒä¸€pathï¼‰!
- å¯¼è‡´cosine simæé«˜

æ— PCA:
- ç›´æ¥åœ¨441ç»´ridge spaceåšCCA
- å¤§é‡å†—ä½™ç»´åº¦å‚ä¸è®¡ç®—
```

---

## ğŸ› ï¸ ç«‹å³å®æ–½çš„ä»£ç ä¿®æ”¹

### æ–¹æ¡ˆAï¼šæœ€å°ä¿®æ”¹ï¼ˆæ¨èé¦–æ¬¡å°è¯•ï¼‰

åœ¨`analysis/cca_alignment.py`çš„CCA sectionä¹‹å‰æ·»åŠ ï¼š

```python
# After concatenating X and Y, before CCA

# =========================================================================
# LEGACY-INSPIRED PREPROCESSING
# =========================================================================
print("\n" + "-"*40)
print("APPLYING LEGACY PREPROCESSING")
print("-"*40)

from sklearn.decomposition import PCA

# 1. Noise injection (regularization)
noise_scale = 0.001
print(f"  Injecting noise (Â±{noise_scale})...")
noise_x = np.random.uniform(-noise_scale, noise_scale, X.shape)
noise_y = np.random.uniform(-noise_scale, noise_scale, Y.shape)
X = X.astype(np.float64) + noise_x
Y = Y.astype(np.float64) + noise_y

# 2. PCA preprocessing
print("  Applying PCA...")
pca_x = PCA()
X_transformed = pca_x.fit_transform(X)
cumsum_x = pca_x.explained_variance_ratio_.cumsum()
n_comp_x_95 = np.searchsorted(cumsum_x, 0.95) + 1
print(f"  X: {X.shape} â†’ PCA â†’ {X_transformed.shape}")
print(f"  X: {n_comp_x_95} components explain 95% variance")

pca_y = PCA()
Y_transformed = pca_y.fit_transform(Y)
cumsum_y = pca_y.explained_variance_ratio_.cumsum()
n_comp_y_95 = np.searchsorted(cumsum_y, 0.95) + 1
print(f"  Y: {Y.shape} â†’ PCA â†’ {Y_transformed.shape}")
print(f"  Y: {n_comp_y_95} components explain 95% variance")

# Use transformed data for CCA
X = X_transformed
Y = Y_transformed

print("-"*40 + "\n")
```

### æ–¹æ¡ˆBï¼šå®Œæ•´é‡æ„ï¼ˆä¸Legacyå®Œå…¨å¯¹é½ï¼‰

ä¿®æ”¹æ•°æ®ç»„ç»‡æ–¹å¼ï¼Œä½¿ç”¨cycle-level aggregationï¼š

```python
# ä¿®æ”¹BUILD DATA MATRICES section

cycle_hidden_means = []
cycle_ridge_vecs = []

for i in range(num_cycles):
    h_cycle = cycles_hidden[i]
    
    # Average hidden states across the cycle
    if len(h_cycle) > 0:
        h_mean = np.mean(h_cycle, axis=0)
    else:
        h_mean = np.zeros(256)
    
    cycle_hidden_means.append(h_mean)
    
    # Compute ridge (one per cycle)
    ridge_vec = build_ridge_vector(path_tile)
    cycle_ridge_vecs.append(ridge_vec)

X = np.array(cycle_hidden_means)  # (N_cycles, 256)
Y = np.array(cycle_ridge_vecs)     # (N_cycles, 441)

# ç„¶ååº”ç”¨å™ªå£°+PCA+CCA
```

---

## ğŸ“Œ å…³é”®å‘ç°æ±‡æ€»

### é—®é¢˜æ ¹æº

**Currentç¨‹åºridgeç›¸ä¼¼åº¦æé«˜(0.9989)çš„åŸå› **ï¼š

1. âŒ **åŒé‡å¯¹é½**ï¼šå…ˆå¯¹é½åˆ°(0,0)ï¼Œridgeå†…éƒ¨å†å¯¹é½åˆ°(10,10) â†’ æ‰€æœ‰ridgeå›¾åƒå‡ ä¹identical
2. âŒ **æ— PCAé™ç»´**ï¼š441ç»´ä¸­çš„å†—ä½™dominates CCA
3. âŒ **Timestep-levelé‡‡æ ·**ï¼šåŒä¸€cycleçš„å¤šä¸ªtimestepsäº§ç”Ÿå®Œå…¨ç›¸åŒçš„ridge â†’ å¤§é‡duplicate samples
4. âŒ **æ— å™ªå£°regularization**ï¼šæ•°å€¼å…±çº¿æ€§artifacts

### LegacyæˆåŠŸçš„å…³é”®

1. âœ… PCAæå–meaningful features
2. âœ… å™ªå£°æ‰“ç ´perfect collinearity  
3. âœ… Cycle-level aggregationï¼ˆæ¯ä¸ªæ ·æœ¬ä»£è¡¨ä¸åŒbehaviorï¼‰
4. âœ… æ­£ç¡®çš„Ridgeå¯¹é½æ–¹å¼

---

## ğŸš€ æ¨èè¡ŒåŠ¨è®¡åˆ’

### Phase 1: å¿«é€ŸéªŒè¯ï¼ˆ1-2å°æ—¶ï¼‰

1. åœ¨Currentç¨‹åºä¸­æ·»åŠ å™ªå£°+PCAï¼ˆæ–¹æ¡ˆAï¼‰
2. è¿è¡Œ`bash ./run_analysis_tuning_filtered.sh --skip-pkd`
3. è§‚å¯Ÿridge cosine similarityæ˜¯å¦é™ä½
4. æ£€æŸ¥CCA correlation distributionæ˜¯å¦æ›´spread

### Phase 2: æ·±åº¦ä¿®å¤ï¼ˆå¦‚æœPhase 1ä¸å¤Ÿï¼‰

1. ä¿®æ­£double alignmenté—®é¢˜
2. æ”¹ä¸ºcycle-level aggregationï¼ˆæ–¹æ¡ˆBï¼‰
3. å…¨é¢å¯¹é½Legacyçš„æ•°æ®pipeline

### Phase 3: éªŒè¯å’Œæ–‡æ¡£åŒ–

1. å¯¹æ¯”æ–°æ—§ç»“æœ
2. è®°å½•æ”¹è¿›æ•ˆæœ
3. æ›´æ–°åˆ†ææ–‡æ¡£

---

## ğŸ“š å‚è€ƒ

- **Legacyç¨‹åº**: `experiments/paper_exp/e_r67_cca_compare.py`
- **Currentç¨‹åº**: `analysis/cca_alignment.py`
- **Ridgeå®ç°**: `analysis/ridge_embedding.py`
- **ç›¸å…³è®ºæ–‡**: "Preserved neural dynamics across animals performing similar behaviour" (CCAæ–¹æ³•æ¥æº)

---

## ğŸ’¬ ç»“è®º

Legacyç¨‹åºçš„æˆåŠŸä¸æ˜¯å¶ç„¶çš„ - å®ƒé‡‡ç”¨äº†**æ­£ç¡®çš„æ•°æ®é¢„å¤„ç†pipeline**ï¼š

```
Raw Data â†’ Noise Injection â†’ PCA â†’ CCA â†’ High-quality results
```

Currentç¨‹åºç¼ºå°‘ä¸­é—´ä¸¤æ­¥ï¼Œå¯¼è‡´ï¼š
- Ridge embeddingsè¿‡åº¦ç›¸ä¼¼
- CCAç»“æœå¯èƒ½æ˜¯artifactsè€ŒéçœŸå®alignment

**å»ºè®®ç«‹å³å®æ–½PCA+å™ªå£°æ”¹è¿›**ï¼Œè¿™æ˜¯æå‡åˆ†æè´¨é‡çš„å…³é”®ï¼

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2025-12-21
